‚ùå 53
‚ö†Ô∏è 2
‚ÑπÔ∏è 16
‚ùå
Cell 1:1
Undefined variable 'capture'
[undefinedVariables]
‚ùå
Cell 1:7
Undefined variable 'pip'
[undefinedVariables]
‚ùå
Cell 1:7
Undefined variable 'install'
[undefinedVariables]
‚ùå
Cell 1:7
Undefined variable 'pip3'
[undefinedVariables]
‚ùå
Cell 1:7
Undefined variable 'autoremove'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'pip'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'install'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'torchvision'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'torchaudio'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'xformers'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'index'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'url'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'https'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'download'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'whl'
[undefinedVariables]
‚ùå
Cell 1:8
Undefined variable 'cu128'
[undefinedVariables]
‚ùå
Cell 1:9
Undefined variable 'pip'
[undefinedVariables]
‚ùå
Cell 1:9
Undefined variable 'install'
[undefinedVariables]
‚ùå
Cell 1:9
Undefined variable 'unsloth'
[undefinedVariables]
‚ùå
Cell 1:10
Undefined variable 'pip'
[undefinedVariables]
‚ùå
Cell 1:10
Undefined variable 'install'
[undefinedVariables]
‚ùå
Cell 1:10
Undefined variable 'transformers'
[undefinedVariables]
‚ùå
Cell 1:11
Undefined variable 'pip'
[undefinedVariables]
‚ùå
Cell 1:11
Undefined variable 'install'
[undefinedVariables]
‚ùå
Cell 1:11
Undefined variable 'no'
[undefinedVariables]
‚ùå
Cell 1:11
Undefined variable 'deps'
[undefinedVariables]
‚ùå
Cell 1:11
Undefined variable 'trl'
[undefinedVariables]
‚ùå
Cell 2:1
Undefined variable 'unsloth'
[undefinedVariables]
‚ùå
Cell 3:2
Undefined variable 'unsloth'
[undefinedVariables]
‚ùå
Cell 3:7
Undefined variable 'datasets'
[undefinedVariables]
‚ùå
Cell 3:8
Undefined variable 'trl'
[undefinedVariables]
‚ùå
Cell 3:35
Undefined variable 'formatted_data'
[undefinedVariables]
‚ùå
Cell 4:6
Undefined variable 'SFTConfig'
[undefinedVariables]
‚ùå
Cell 4:7
Undefined variable 'output_direct'
[undefinedVariables]
‚ùå
Cell 4:7
Unexpected indent
[indentationErrors]
‚ùå
Cell 4:9
Undefined variable 'batch_size'
[undefinedVariables]
‚ùå
Cell 4:10
Undefined variable 'gr_steps'
[undefinedVariables]
‚ùå
Cell 4:11
Undefined variable 'num_epochs'
[undefinedVariables]
‚ùå
Cell 4:33
Undefined variable 'train_dataset'
[undefinedVariables]
‚ùå
Cell 5:5
Undefined variable 'SFTTrainer'
[undefinedVariables]
‚ùå
Cell 5:6
Unexpected indent
[indentationErrors]
‚ùå
Cell 5:12
Undefined variable 'training_args'
[undefinedVariables]
‚ùå
Cell 5:32
Undefined variable 'start_gpu_memory'
[undefinedVariables]
‚ùå
Cell 5:33
Undefined variable 'max_memory'
[undefinedVariables]
‚ùå
Cell 5:34
Undefined variable 'max_memory'
[undefinedVariables]
‚ùå
Cell 5:36
Undefined variable 'f'
[undefinedVariables]
‚ùå
Cell 7:3
Undefined variable 'pandas'
[undefinedVariables]
‚ùå
Cell 7:4
Undefined variable 'unsloth'
[undefinedVariables]
‚ùå
Cell 7:17
Undefined variable 'r'
[undefinedVariables]
‚ùå
Cell 7:20
Undefined variable 'r'
[undefinedVariables]
‚ùå
Cell 7:25
Undefined variable 'f'
[undefinedVariables]
‚ùå
Cell 7:27
Unexpected indent
[indentationErrors]
‚ùå
Cell 7:33
Undefined variable 'f'
[undefinedVariables]
‚ö†Ô∏è
Cell 3:22
Function 'prepare_training_data' appears to compute a value but has no return statement
[missingReturn]
‚ö†Ô∏è
Cell 7:36
Function 'generate_4_predictions' appears to compute a value but has no return statement
[missingReturn]
‚ÑπÔ∏è
Cell 1:2
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 2:1
Imported 'FastLanguageModel' is unused
[importIssues]
‚ÑπÔ∏è
Cell 2:2
Imported 'torch' is unused
[importIssues]
‚ÑπÔ∏è
Cell 3:2
Imported 'FastLanguageModel' is unused
[importIssues]
‚ÑπÔ∏è
Cell 3:6
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 3:7
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 3:7
Imported 'Dataset' is unused
[importIssues]
‚ÑπÔ∏è
Cell 3:8
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 3:8
Imported 'SFTTrainer' is unused
[importIssues]
‚ÑπÔ∏è
Cell 3:8
Imported 'SFTConfig' is unused
[importIssues]
‚ÑπÔ∏è
Cell 3:9
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 3:9
Imported 'torch' is unused
[importIssues]
‚ÑπÔ∏è
Cell 5:20
Import statement should be at the top of the file/cell
[importIssues]
‚ÑπÔ∏è
Cell 7:1
Imported 'os' is unused
[importIssues]
‚ÑπÔ∏è
Cell 7:3
Imported 'pd' is unused
[importIssues]
‚ÑπÔ∏è
Cell 7:5
Imported 'torch' is unused
[importIssues]



cells:
Cell 1:
```python
%%capture
import os

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   # see issue #152
os.environ["CUDA_VISIBLE_DEVICES"]="0"

!pip install pip3-autoremove
!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128
!pip install unsloth
!pip install transformers==4.56.2
!pip install --no-deps trl==0.22.2
```

Cell 2:
```python
from unsloth import FastLanguageModel
import torch


Train_7b = False
model_name = "unsloth/Qwen2.5-1.5B-Instruct"  # ‚Üê MAIN CHANGE
output_direct = "./qwen2.5-1.5b-telelogs-lora-max-seq"
lora_r = 8
lora_alp = 8
batch_size = 16
gr_steps = 2
num_epochs = 3

# MODEL CONFIGURATION
if Train_7b:
    model_name = "unsloth/Qwen2.5-7B-Instruct"  # Using Unsloth's version
    output_direct = "./qwen2.5-7b-telelogs-lora-max-seq"
    lora_r = 16
    lora_alp = 16
    batch_size = 4
    gr_steps = 4
    num_epochs = 2
```
Cell 3:
```python 
# stage2a_finetune_7b_UNSLOTH.py
from unsloth import FastLanguageModel
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import json
from datasets import Dataset
from trl import SFTTrainer, SFTConfig
import torch

# ============================================================================
# CONFIGURATION
# ============================================================================
TELELOG_WITH_REASONING_FILE = '/kaggle/input/telelogs/telelogs_with_reasoning (4).jsonl'
max_seq_length = 5120  # Your requirement
dtype = None  # Auto-detect
load_in_4bit = True

# ============================================================================
# DATA PREPARATION
# ============================================================================
def prepare_training_data(jsonl_file):
    """Load enriched dataset and format for training"""
    data = []
    with open(jsonl_file, 'r') as f:
        for line in f:
            obj = json.loads(line)
            # Skip error entries
            if not obj.get('reasoning', '').startswith('[ERROR:'):
                data.append(obj)
    
    print(f"‚úÖ Loaded {len(data)} valid samples")
    
    # Format as instruction dataset
    formatted_data = []
    for item in data:
        if item['reasoning'].startswith("[ERROR:"):
            continue
            
        text = f"""<|im_start|>system
You are a telecom network troubleshooting expert. Analyze 5G network data and provide detailed reasoning before selecting the root cause.<|im_end|>
<|im_start|>user
{item['question']}<|im_end|>
<|im_start|>assistant
{item['reasoning']}
Therefore, the most likely root cause is \\boxed{{{item['answer']}}}<|im_end|>"""
        formatted_data.append({"text": text})
    
    return Dataset.from_list(formatted_data)

# ============================================================================
# MODEL LOADING WITH UNSLOTH
# ============================================================================
print("=" * 60)
print("LOADING MODEL WITH UNSLOTH OPTIMIZATIONS")
print("=" * 60)

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

print("‚úÖ Model loaded!")

# ============================================================================
# ADD LORA ADAPTERS
# ============================================================================
model = FastLanguageModel.get_peft_model(
    model,
    r=lora_r,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=lora_alp,
    lora_dropout=0,  # Unsloth optimizes this internally
    bias="none",
    use_gradient_checkpointing="unsloth",  # Unsloth's optimized checkpointing
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

print("‚úÖ LoRA adapters added!")

# Print trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {trainable_params:,} || All params: {all_params:,} || Trainable%: {100 * trainable_params / all_params:.2f}%")

# ============================================================================
# LOAD TRAINING DATA
# ============================================================================
train_dataset = prepare_training_data(TELELOG_WITH_REASONING_FILE)
print(f"‚úÖ Dataset loaded: {len(train_dataset)} samples\n")
```
Cell 4:
```python
print("="*60)
# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================

training_args = SFTConfig(
    output_dir=output_direct,
    
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=gr_steps,
    num_train_epochs=num_epochs,
    learning_rate=2e-4,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    warmup_steps=5,
    logging_steps=1,
    logging_first_step=True,
    save_strategy="epoch",
    save_total_limit=2,
    bf16=0,
    fp16=1,
    max_seq_length=max_seq_length,
    dataset_text_field="text",
    packing=False,  # Unsloth handles this internally better
    seed=3407,
    report_to=[],
)

# ============================================================================
# CALCULATE TRAINING STATS
# ============================================================================
total_samples = len(train_dataset)
effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps
steps_per_epoch = total_samples // effective_batch_size
total_steps = steps_per_epoch * training_args.num_train_epochs

print(f"{'='*60}")
print(f"TRAINING CONFIGURATION WITH UNSLOTH")
print(f"{'='*60}")
print(f"Total samples:           {total_samples}")
print(f"Effective batch size:    {effective_batch_size}")
print(f"Steps per epoch:         {steps_per_epoch}")
print(f"Total epochs:            {training_args.num_train_epochs}")
print(f"Total training steps:    {total_steps}")
print(f"Expected speedup:        5-7x faster than standard training")
print(f"{'='*60}\n")

# Show memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.\n")
```
Cell 5:
```python

# ============================================================================
# CREATE TRAINER
# ============================================================================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    packing=False,
    args=training_args,
)

# ============================================================================
# TRAIN
# ============================================================================
print("üöÄ Starting training with Unsloth optimizations...\n")

import time
start_time = time.time()
trainer_stats = trainer.train()

elapsed_time = time.time() - start_time
hours = int(elapsed_time // 3600)
minutes = int((elapsed_time % 3600) // 60)

# ============================================================================
# SHOW FINAL STATS
# ============================================================================
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_training = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
training_percentage = round(used_memory_for_training / max_memory * 100, 3)

print(f"\n{'='*60}")
print(f"‚úÖ TRAINING COMPLETED!")
print(f"{'='*60}")
print(f"Training time:                    {hours}h {minutes}m ({trainer_stats.metrics['train_runtime']:.2f}s)")
print(f"Peak reserved memory:             {used_memory} GB")
print(f"Peak reserved memory for training: {used_memory_for_training} GB")
print(f"Peak memory % of max:             {used_percentage}%")
print(f"Training memory % of max:         {training_percentage}%")
print(f"{'='*60}\n")

# ============================================================================
# SAVE MODEL
# ============================================================================
print("Saving LoRA adapters...")
vers = '7' if Train_7b else '1.5'
model.save_pretrained(f"./qwen2.5-{vers}b-telelogs-lora-final")
tokenizer.save_pretrained(f"./qwen2.5-{vers}b-telelogs-lora-final")
print(f"‚úÖ LoRA adapters saved to ./qwen2.5-{vers}b-telelogs-lora-final")

# Optional: Save merged 16-bit model for inference
"""
print("\nSaving merged 16-bit model for faster inference...")
model.save_pretrained_merged(
    "./qwen2.5-7b-telelogs-merged-16bit",
    tokenizer,
    save_method="merged_16bit",
)
print("‚úÖ 16-bit merged model saved to ./qwen2.5-7b-telelogs-merged-16bit")
"""
print("\n" + "="*60)
print("ALL DONE! üéâ")
```
Cell 6:
```python
print('Hello, world!')
```
Cell 7:
```python
import os
import re
import pandas as pd
from unsloth import FastLanguageModel
import torch

# Configuration
MODEL_7B_PATH = "./qwen2.5-7b-telelogs-lora-final"
MODEL_1_5B_PATH = "./qwen2.5-1.5b-telelogs-lora-final"

# MD:
#: ### Helper Functions

# CODE:
def extract_answer(text):
    """Extract answer from \\boxed{Ck} format"""
    match = re.search(r'\\boxed\{(C\d+)\}', text)
    if match:
        return match.group(1)
    matches = re.findall(r'\b(C[1-8])\b', text)
    return matches[-1] if matches else "C1"

def load_unsloth_model(model_path, max_seq_length=5120):
    """Load Unsloth fine-tuned model for inference"""
    print(f"Loading model from {model_path}...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)
    print(f"‚úÖ Model loaded")
    return model, tokenizer

def generate_4_predictions(model, tokenizer, question):
    """Generate 4 diverse predictions"""
    predictions = []
    configs = [
        {"temperature": 0.7, "top_p": 0.9, "seed": 42},
        {"temperature": 0.8, "top_p": 0.95, "seed": 7},
        {"temperature": 0.6, "top_p": 0.85, "seed": 3407},
        {"temperature": 0.75, "top_p": 0.92, "seed": 69}
    ]
    
    for config in configs:
        messages = [{"role": "user", "content": question}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        torch.manual_seed(config["seed"])
        with torch.no_grad():
            outputs = model.generate(
                **inputs, max_new_tokens=1024, temperature=config["temperature"],
                top_p=config["top_p"], do_sample=True, use_cache=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        answer = extract_answer(response)
        full_response = response.strip()
        if "\\boxed{" not in full_response:
            full_response += f" \\boxed{{{answer}}}"
        predictions.append(full_response)
    
    return predictions

# MD:
#: ### Load Models

# CODE:
print("="*60)
print("LOADING MODELS")
print("="*60)
"""
model_7b, tokenizer_7b = load_unsloth_model(MODEL_7B_PATH)
model_1_5b, tokenizer_1_5b = load_unsloth_model(MODEL_1_5B_PATH)

# MD:
#: ### Load Test Data

# CODE:
# Option 1: From CSV
try:
    test_df = pd.read_csv('test.csv')
    test_questions = test_df['question'].tolist()
    test_ids = test_df['ID'].tolist()
except:
    # Option 2: From HuggingFace
    from datasets import load_dataset
    hf_token = os.environ.get('HF_TOKEN')
    if hf_token:
        from huggingface_hub import login
        login(token=hf_token)
    test_dataset = load_dataset("netop/TeleLogs", split="test", use_auth_token=True)
    test_questions = [s['question'] for s in test_dataset]
    test_ids = [s.get('id', f'test_{i}') for i, s in enumerate(test_dataset)]

print(f"‚úÖ Loaded {len(test_questions)} test questions")

# MD:
#: ### Generate Predictions

# CODE:
submission_rows = []

for idx, (test_id, question) in enumerate(zip(test_ids, test_questions)):
    print(f"\nProcessing [{idx+1}/{len(test_questions)}] {test_id}...")
    
    predictions_7b = generate_4_predictions(model_7b, tokenizer_7b, question)
    predictions_1_5b = generate_4_predictions(model_1_5b, tokenizer_1_5b, question)
    
    for i in range(4):
        submission_rows.append({
            'ID': f'{test_id}_{i+1}',
            'Qwen3-32B': 'placeholder',
            'Qwen2.5-7B-Instruct': predictions_7b[i],
            'Qwen2.5-1.5B-Instruct': predictions_1_5b[i]
        })
    
    if (idx + 1) % 10 == 0:
        torch.cuda.empty_cache()

# MD:
#: ### Save Submission

# CODE:
submission_df = pd.DataFrame(submission_rows)
submission_df.to_csv('submission.csv', index=False)

print("\n" + "="*60)
print("SUBMISSION COMPLETE!")
print("="*60)
print(f"‚úÖ Saved to submission.csv")
print(f"üìä Total rows: {len(submission_df)}")
print("\nSample:")
print(submission_df.head(10))
"""
# MD:
#: ## Key Changes from Original Stage 3:
#: 
#: 1. **Unsloth Loading**: Uses `FastLanguageModel.from_pretrained()` instead of standard transformers
#: 2. **Fast Inference**: Calls `FastLanguageModel.for_inference(model)` for 2x faster generation
#: 3. **No LoRA merging needed**: Unsloth handles LoRA weights automatically
#: 4. **Memory optimization**: Added `torch.cuda.empty_cache()` every 10 questions
#: 5. **Simplified loading**: No need for `PeftModel.from_pretrained()` - Unsloth handles everything
#: 
#: The rest stays the same: 4 diverse generations per question using different temperature/seed combos.
```